{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Sentence-transformers\n",
    "%pip install re\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Procedure__:\n",
    "  #### 1. Removal of Stopwords\n",
    "  #### 2. Lemetization\n",
    "  #### 3. Removing any non-meaningful words with the help of re module.\n",
    "  \n",
    " Note: Stemming was not performed as it was observed that it was mostly damaging the information which can be used to find similarity between the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/udayb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/udayb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "PATH='Collection//euro_news//para.csv'\n",
    "df=pd.read_csv(PATH,sep='|',names=['Channel','Date','Time','Place','News'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Place</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Euro News</td>\n",
       "      <td>01/01/23</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Outside India</td>\n",
       "      <td>10 year joining European Union Croatia swit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Euro News</td>\n",
       "      <td>01/01/23</td>\n",
       "      <td>Midday</td>\n",
       "      <td>Outside India</td>\n",
       "      <td>half hour 2023 Air Raid Sirens ring across Uk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Euro News</td>\n",
       "      <td>01/01/23</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Outside India</td>\n",
       "      <td>Noble kind word used Pope Francis describe pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Euro News</td>\n",
       "      <td>01/02/23</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Outside India</td>\n",
       "      <td>sign slight easing cost living crisis inflati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Euro News</td>\n",
       "      <td>01/02/23</td>\n",
       "      <td>Midday</td>\n",
       "      <td>Outside India</td>\n",
       "      <td>foreign half million worker expected strike U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>Euro News</td>\n",
       "      <td>31/10/23</td>\n",
       "      <td>Midday</td>\n",
       "      <td>Outside India</td>\n",
       "      <td>Israeli air strike continue dark hundred thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>Euro News</td>\n",
       "      <td>31/10/23</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Outside India</td>\n",
       "      <td>defiant Benjamin Netanyahu ruled ceasefire Ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>Euro News</td>\n",
       "      <td>31/12/22</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Outside India</td>\n",
       "      <td>European leader commemorate former Pope Ben...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>Euro News</td>\n",
       "      <td>31/12/22</td>\n",
       "      <td>Midday</td>\n",
       "      <td>Outside India</td>\n",
       "      <td>former Pope Benedict XVI first pontiff 600 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>Euro News</td>\n",
       "      <td>31/12/22</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Outside India</td>\n",
       "      <td>China prepares reopen border amid surge cov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1536 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel      Date     Time          Place  \\\n",
       "0     Euro News  01/01/23  Evening  Outside India   \n",
       "1     Euro News  01/01/23   Midday  Outside India   \n",
       "2     Euro News  01/01/23  Morning  Outside India   \n",
       "3     Euro News  01/02/23  Evening  Outside India   \n",
       "4     Euro News  01/02/23   Midday  Outside India   \n",
       "...         ...       ...      ...            ...   \n",
       "1531  Euro News  31/10/23   Midday  Outside India   \n",
       "1532  Euro News  31/10/23  Morning  Outside India   \n",
       "1533  Euro News  31/12/22  Evening  Outside India   \n",
       "1534  Euro News  31/12/22   Midday  Outside India   \n",
       "1535  Euro News  31/12/22  Morning  Outside India   \n",
       "\n",
       "                                                   News  \n",
       "0        10 year joining European Union Croatia swit...  \n",
       "1      half hour 2023 Air Raid Sirens ring across Uk...  \n",
       "2      Noble kind word used Pope Francis describe pr...  \n",
       "3      sign slight easing cost living crisis inflati...  \n",
       "4      foreign half million worker expected strike U...  \n",
       "...                                                 ...  \n",
       "1531   Israeli air strike continue dark hundred thou...  \n",
       "1532   defiant Benjamin Netanyahu ruled ceasefire Ga...  \n",
       "1533     European leader commemorate former Pope Ben...  \n",
       "1534     former Pope Benedict XVI first pontiff 600 ...  \n",
       "1535     China prepares reopen border amid surge cov...  \n",
       "\n",
       "[1536 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "# Creating  the object for lemmatisation\n",
    "lemt = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "for ind, row  in df.iterrows():\n",
    "\n",
    "\ttext=df.loc[(ind,'News')]\n",
    "\twords=nltk.word_tokenize(\ttext\t)\n",
    "\t\n",
    "\tNEW_HDL=\"\"\n",
    "\tfor word in words:\n",
    "\t\tif word not in stop_words:\n",
    "\t\t\tNEW_HDL=NEW_HDL+' '+lemt.lemmatize(word)\n",
    "\n",
    "\t# Removing strings which are inside the parantheses from the news headline \n",
    "\tNEW_HDL=re.sub('\\[[a-zA-Z0-9,\\' ]+\\]',' ',NEW_HDL)\n",
    "\t\n",
    "\tdf.loc[(ind,'News')]=NEW_HDL\n",
    "\t\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for similarity in the News Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the below cell, we have made 4 major Methods which are later used to find similarity in the news which are released in the day.\n",
    "\n",
    "### A Brief description of the methods is given below:\n",
    "---\n",
    "  - ####  _EMBED\\_COS_\n",
    "    - This embeds the news headlines of Morning, Midday and Evening by the help of the model [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 \"Click to know more\").\n",
    "    - It gives the embeddings in the form of a numpy array containing the values for Morning, Midday and Evening.\n",
    "  ---\n",
    "  \n",
    "  - ####  _EMBED\\_SEM_\n",
    "    - This embeds the news headlines of Morning, Midday and Evening by the help of the model [clips/mfaq]( https://huggingface.co/clips/mfaq \"Click to know more\").\n",
    "    - Similar to the previous one, this gives Embeddings in the form of a numpy array consisting of the values for Morning, Midday and Evening.\n",
    "  - ####  _COS\\_SIM_\n",
    "    - Here, we take out the _Cosine Similarity_ with the help of util library from the Sentence transformers library.\n",
    "  - #### _SEM_SIM_\n",
    "    - We take out the Semantic Similarity with the help of util library from the Sentence Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer as sen_tr, util\n",
    "\n",
    "def EMBED_COS(news_list:dict):\n",
    "  \"\"\"\n",
    "    - The model which we have used here for Embedding is all-MiniLM-L6-v2.\n",
    "    - This gives an nd array of Embeddings in the format of:\n",
    "        [  Morning , Midday , Evening ]\n",
    "  \"\"\"\n",
    "\n",
    "  # Creating an object for the model\n",
    "  model = sen_tr('all-MiniLM-L6-v2')\n",
    "  \n",
    "  # Creating Embeddings for different timings of the day\n",
    "  emb0 = model.encode(news_list['Morning'])\n",
    "  emb1 = model.encode(news_list['Midday'])\n",
    "  emb2 = model.encode(news_list['Evening'])\n",
    "  \n",
    "  return np.array([ emb0 ,emb1 , emb2 ])\n",
    "\n",
    "\n",
    "def EMBED_SEM(  news_list:dict ):\n",
    "  \"\"\"\n",
    "    - The model which we have used here for Embedding is clips/mfaq.\n",
    "    - This gives an nd array of Embeddings in the format of:\n",
    "      [ Morning , Midday  , Evening ]\n",
    "  \"\"\"\n",
    "\n",
    "  model=sen_tr('clips/mfaq')\n",
    "\n",
    "  emb0=model.encode(news_list['Morning'])\n",
    "  emb1=model.encode(news_list['Midday'])\n",
    "  emb2=model.encode(news_list['Evening'])\n",
    "\n",
    "  return np.array(  [ emb0  , emb1  , emb2  ]  )\n",
    "\n",
    "\n",
    "def COS_SIM(embed: dict):\n",
    "  \"\"\"\n",
    "    This function returns the cosine-similarity of different timings in the format:\n",
    "\n",
    "  \"\"\"\n",
    "  def rel(val):\n",
    "    return val*100\n",
    "  \n",
    "  ans={'Morning-Midday':0,'Midday-Evening':0,'Morning-Evening':0}\n",
    "\n",
    "  ans['Morning-Midday'] = rel(  util.cos_sim(embed[0],embed[1]  )[0,0].item())\n",
    "  ans['Morning-Evening'] = rel(  util.cos_sim(embed[0],embed[2]  )[0,0].item())\n",
    "  ans['Midday-Evening'] = rel(  util.cos_sim(embed[1],embed[2]  )[0,0].item())\n",
    "\n",
    "  return ans\n",
    "\n",
    "\n",
    "def SEM_SIM(embed: dict):\n",
    "\n",
    "  def rel(val):\n",
    "    return val*100\n",
    "  \n",
    "  ans={'Morning-Midday':0,'Midday-Evening':0,'Morning-Evening':0}\n",
    "\n",
    "  ans['Morning-Midday']   = rel(  ( util.semantic_search( embed[0] ,  embed[1] )[0][0] )['score'] )\n",
    "  ans['Morning-Evening']  = rel(  ( util.semantic_search( embed[0] ,  embed[2] )[0][0] )['score'] )\n",
    "  ans['Midday-Evening']   = rel(  ( util.semantic_search( embed[1] ,  embed[2] )[0][0] )['score'] )\n",
    "\n",
    "  return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code takes 20 min Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udayb/.local/lib/python3.11/site-packages/transformers/configuration_utils.py:381: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "un_date=df['Date'].unique()\n",
    "\n",
    "data_similar={'Date':[],'Technique':[],'Morning-Midday':[],'Midday-Evening':[],'Morning-Evening':[]}\n",
    "\n",
    "\n",
    "for DATE in un_date:\n",
    "\n",
    "\n",
    "  df_day=df[df['Date']==DATE]\n",
    "\n",
    "  news={\n",
    "          'Morning':  '',\n",
    "          'Midday' :  '',\n",
    "          'Evening':  ''\n",
    "        }\n",
    "\n",
    "  for ind, row in df_day.iterrows():\n",
    "    news[row['Time']]=row['News']\n",
    "  \n",
    "  \n",
    "  embed_cos = EMBED_COS(news);          \n",
    "  dt_cos  = COS_SIM(embed_cos)\n",
    "\n",
    "  embed_sem= EMBED_SEM(news)\n",
    "  dt_sem  = SEM_SIM(embed_sem) \n",
    "\n",
    "  print(1)\n",
    "\n",
    "  data_similar['Technique'].append('Cosine Similarity');  data_similar['Date'].append(DATE);  data_similar['Morning-Midday'].append(dt_cos['Morning-Midday']);  data_similar['Midday-Evening'].append(dt_cos['Midday-Evening']);  data_similar['Morning-Evening'].append(dt_cos['Morning-Evening'])\n",
    "  data_similar['Technique'].append('Semantic Similarity');  data_similar['Date'].append(DATE);  data_similar['Morning-Midday'].append(dt_sem['Morning-Midday']);  data_similar['Midday-Evening'].append(dt_sem['Midday-Evening']);  data_similar['Morning-Evening'].append(dt_sem['Morning-Evening'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7643445879220963"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_df=pd.DataFrame(data_similar)\n",
    "per_df['Morning-Evening'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Morning-Midday</th>\n",
       "      <th>Midday-Evening</th>\n",
       "      <th>Morning-Evening</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/23</td>\n",
       "      <td>Cosine Similarity</td>\n",
       "      <td>73.351413</td>\n",
       "      <td>44.613236</td>\n",
       "      <td>34.173316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/23</td>\n",
       "      <td>Semantic Similarity</td>\n",
       "      <td>96.062630</td>\n",
       "      <td>97.445554</td>\n",
       "      <td>95.561874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/02/23</td>\n",
       "      <td>Cosine Similarity</td>\n",
       "      <td>57.157195</td>\n",
       "      <td>59.162945</td>\n",
       "      <td>42.400160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/02/23</td>\n",
       "      <td>Semantic Similarity</td>\n",
       "      <td>98.220742</td>\n",
       "      <td>98.225093</td>\n",
       "      <td>98.085129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/03/23</td>\n",
       "      <td>Cosine Similarity</td>\n",
       "      <td>73.402280</td>\n",
       "      <td>48.875538</td>\n",
       "      <td>48.308423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>31/10/22</td>\n",
       "      <td>Semantic Similarity</td>\n",
       "      <td>99.828959</td>\n",
       "      <td>99.141967</td>\n",
       "      <td>99.205130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>31/10/23</td>\n",
       "      <td>Cosine Similarity</td>\n",
       "      <td>86.921757</td>\n",
       "      <td>95.947510</td>\n",
       "      <td>85.745031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>31/10/23</td>\n",
       "      <td>Semantic Similarity</td>\n",
       "      <td>99.000019</td>\n",
       "      <td>99.553859</td>\n",
       "      <td>99.041295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>31/12/22</td>\n",
       "      <td>Cosine Similarity</td>\n",
       "      <td>14.287755</td>\n",
       "      <td>73.257643</td>\n",
       "      <td>22.974724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>31/12/22</td>\n",
       "      <td>Semantic Similarity</td>\n",
       "      <td>95.318639</td>\n",
       "      <td>98.374921</td>\n",
       "      <td>97.092074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1024 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date            Technique  Morning-Midday  Midday-Evening  \\\n",
       "0     01/01/23    Cosine Similarity       73.351413       44.613236   \n",
       "1     01/01/23  Semantic Similarity       96.062630       97.445554   \n",
       "2     01/02/23    Cosine Similarity       57.157195       59.162945   \n",
       "3     01/02/23  Semantic Similarity       98.220742       98.225093   \n",
       "4     01/03/23    Cosine Similarity       73.402280       48.875538   \n",
       "...        ...                  ...             ...             ...   \n",
       "1019  31/10/22  Semantic Similarity       99.828959       99.141967   \n",
       "1020  31/10/23    Cosine Similarity       86.921757       95.947510   \n",
       "1021  31/10/23  Semantic Similarity       99.000019       99.553859   \n",
       "1022  31/12/22    Cosine Similarity       14.287755       73.257643   \n",
       "1023  31/12/22  Semantic Similarity       95.318639       98.374921   \n",
       "\n",
       "      Morning-Evening  \n",
       "0           34.173316  \n",
       "1           95.561874  \n",
       "2           42.400160  \n",
       "3           98.085129  \n",
       "4           48.308423  \n",
       "...               ...  \n",
       "1019        99.205130  \n",
       "1020        85.745031  \n",
       "1021        99.041295  \n",
       "1022        22.974724  \n",
       "1023        97.092074  \n",
       "\n",
       "[1024 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_df.to_csv('result/similarity.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
